{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the IMDB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i couldn't initialy open the dataset so i found this online. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "\n",
    "# save np.load\n",
    "np_load_old = np.load\n",
    "\n",
    "# modify the default parameters of np.load\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "# call load_data with allow_pickle implicitly set to true\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "\n",
    "# restore np.load for future normal usage\n",
    "np.load = np_load_old\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'keras.datasets.imdb' from '/home/cardosoo/venv/lib/python3.5/site-packages/keras/datasets/imdb.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "top 10,000 most frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([max(sequence) for sequence in train_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
      "1646592/1641221 [==============================] - 2s 1us/step\n"
     ]
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'compendium': 48458,\n",
       " 'engineer': 8034,\n",
       " \"gm's\": 54879,\n",
       " 'looonnnggg': 80846,\n",
       " 'yeardley': 33750,\n",
       " 'guests': 5537,\n",
       " 'dickson': 66768,\n",
       " 'thimig': 79770,\n",
       " 'chatham': 58346,\n",
       " 'heavenly': 8907,\n",
       " 'editorialised': 83841,\n",
       " 'opéra': 55817,\n",
       " 'purporting': 47689,\n",
       " '10mil': 74807,\n",
       " 'chant': 13140,\n",
       " 'apps': 43525,\n",
       " \"cheung's\": 37465,\n",
       " 'capas': 83561,\n",
       " \"sense'\": 45661,\n",
       " 'loather': 84828,\n",
       " 'papamichael': 53801,\n",
       " 'shiner': 27555,\n",
       " 'declaim': 74791,\n",
       " 'judy': 4421,\n",
       " 'stomps': 26030,\n",
       " 'maudlin': 13746,\n",
       " 'adeptness': 81038,\n",
       " 'wise\\x85': 68303,\n",
       " 'riker': 18367,\n",
       " 'fraud': 7862,\n",
       " 'chemicals': 17278,\n",
       " 'arvide': 77963,\n",
       " 'overtone': 36057,\n",
       " 'peasant': 11086,\n",
       " 'brylcreem': 87545,\n",
       " 'philosophize': 50343,\n",
       " 'edged': 9996,\n",
       " 'ppfff': 62509,\n",
       " 'smut': 17392,\n",
       " 'albertson': 39087,\n",
       " 'blackburn': 51750,\n",
       " 'swordsman': 12903,\n",
       " 'horseshit': 87483,\n",
       " \"superhero's\": 64721,\n",
       " \"'as\": 40357,\n",
       " 'thereby': 7660,\n",
       " \"qi's\": 64204,\n",
       " 'redistribute': 78840,\n",
       " 'storytelling': 2801,\n",
       " 'nobuhiro': 34751,\n",
       " 'tripods': 25143,\n",
       " 'interfernce': 57121,\n",
       " 'lindum': 40111,\n",
       " 'kasugi': 81888,\n",
       " 'dandridge': 71631,\n",
       " 'transliteration': 58565,\n",
       " 'pretext': 13462,\n",
       " 'holloway': 16917,\n",
       " 'mohawk': 26631,\n",
       " 'preference': 12262,\n",
       " 'unevenly': 29991,\n",
       " 'take': 190,\n",
       " 'blore': 25718,\n",
       " 'wields': 22660,\n",
       " 'hemsley': 53600,\n",
       " 'became': 874,\n",
       " \"liebermann's\": 60742,\n",
       " '\\x91st': 63142,\n",
       " 'lightsaber': 41480,\n",
       " 'accusing': 27079,\n",
       " \"speakman's\": 64558,\n",
       " 'foop': 58041,\n",
       " 'talkes': 54456,\n",
       " 'sterotype': 85902,\n",
       " 'yor': 45522,\n",
       " 'hunks': 26847,\n",
       " 'toxie': 73642,\n",
       " 'dullllllllllll': 63834,\n",
       " 'eduction': 65820,\n",
       " 'coburn': 8015,\n",
       " 'jaye': 48180,\n",
       " 'palpably': 36439,\n",
       " 'displays': 3853,\n",
       " 'garrigan': 40430,\n",
       " 'cosy': 37996,\n",
       " 'quibble': 13001,\n",
       " 'release': 763,\n",
       " 'regret': 2595,\n",
       " 'hamatova': 86685,\n",
       " 'numbs': 39933,\n",
       " 'where': 118,\n",
       " 'filet': 83404,\n",
       " 'tetsuro': 36204,\n",
       " 'spouted': 33671,\n",
       " 'tirade': 19686,\n",
       " 'malice': 16194,\n",
       " \"serat's\": 51907,\n",
       " 'slugger': 35742,\n",
       " 'flattering': 17253,\n",
       " 'universale': 82716,\n",
       " 'lectern': 43866,\n",
       " 'doxen': 58844,\n",
       " 'denueve': 83234,\n",
       " \"'frankenstein\": 69859,\n",
       " 'kookily': 75011,\n",
       " 'maiko': 43196,\n",
       " 'kapoor': 4026,\n",
       " 'scattergun': 67555,\n",
       " 'accounting': 20771,\n",
       " 'horizons': 17595,\n",
       " 'phillipenes': 52313,\n",
       " 'zant': 50608,\n",
       " 'instinctively': 17681,\n",
       " 'taller': 16976,\n",
       " 'fagin': 8590,\n",
       " 'concur': 18603,\n",
       " 'baaaaaad': 41809,\n",
       " 'podge': 15968,\n",
       " 'unwarranted': 19356,\n",
       " 'moriarty': 13289,\n",
       " 'perp': 40477,\n",
       " 'samey': 45429,\n",
       " 'subduing': 48165,\n",
       " 'arp': 88147,\n",
       " 'juscar': 86372,\n",
       " \"drawer'\": 67330,\n",
       " 'basher': 42088,\n",
       " \"rock's\": 17220,\n",
       " \"nights'\": 42973,\n",
       " 'necromaniac': 51493,\n",
       " 'zoinks': 64430,\n",
       " 'vampish': 36134,\n",
       " 'shets': 65986,\n",
       " 'brough': 85025,\n",
       " 'meritocracy': 37310,\n",
       " 'soporific': 22491,\n",
       " 'sumitra': 26544,\n",
       " 'zhen': 56894,\n",
       " \"chariot's\": 74693,\n",
       " 'baurki': 53989,\n",
       " 'tonka': 49884,\n",
       " 'abvious': 86108,\n",
       " \"may've\": 76936,\n",
       " 'mandala': 73607,\n",
       " \"'signature'\": 82715,\n",
       " 'divorces': 27730,\n",
       " 'ihave': 53124,\n",
       " \"lin's\": 85589,\n",
       " 'anthology\\x97a': 66926,\n",
       " \"'funny\": 49613,\n",
       " 'we´ll': 60073,\n",
       " 'swineherd': 71426,\n",
       " 'buford': 21318,\n",
       " 'lancashire': 48032,\n",
       " 'suspense': 833,\n",
       " \"woolly'\": 46208,\n",
       " 'thouroughly': 40601,\n",
       " 'crackdown': 61702,\n",
       " 'nude': 2513,\n",
       " 'voluntary': 47591,\n",
       " \"orphanage'\": 76307,\n",
       " 'spiralled': 73977,\n",
       " 'makinen': 62144,\n",
       " 'shelbyville': 60059,\n",
       " 'endearment': 28239,\n",
       " 'ventilation': 72594,\n",
       " 'bafta': 12131,\n",
       " 'assuaged': 65137,\n",
       " 'footman': 70989,\n",
       " 'authenticating': 70955,\n",
       " 'muhammad': 43848,\n",
       " 'muggings': 80840,\n",
       " \"boatswain's\": 60317,\n",
       " 'optimally': 43491,\n",
       " \"'waqt\": 65840,\n",
       " \"four'\": 53572,\n",
       " 'allegations': 23570,\n",
       " \"academy's\": 35940,\n",
       " 'cgis': 39436,\n",
       " 'smugness': 19955,\n",
       " 'thinnest': 39093,\n",
       " 'bused': 78208,\n",
       " 'darwin': 26261,\n",
       " 'bumblebee': 46044,\n",
       " 'mortals': 20315,\n",
       " 'violinist': 28086,\n",
       " 'husk': 36575,\n",
       " 'trenholm': 18674,\n",
       " 'doré': 79132,\n",
       " 'zimmerframe': 77092,\n",
       " 'shakers': 49959,\n",
       " 'torino': 66441,\n",
       " \"script's\": 14814,\n",
       " 'peoples': 4839,\n",
       " 'acquaintances': 12983,\n",
       " \"path's\": 64041,\n",
       " 'townhouse': 48007,\n",
       " 'deserve': 1830,\n",
       " 'husbang': 52378,\n",
       " 'prodigies': 65935,\n",
       " \"adrien's\": 38627,\n",
       " 'delicatessen': 33123,\n",
       " 'gacy': 59309,\n",
       " 'clays': 87931,\n",
       " 'secluded': 9911,\n",
       " 'bp': 48023,\n",
       " 'immortally': 81700,\n",
       " 'chiselled': 41558,\n",
       " 'misanthropy': 34979,\n",
       " 'mbbs': 53748,\n",
       " 'ploughs': 84410,\n",
       " 'poofs': 53197,\n",
       " 'standby': 40484,\n",
       " 'unrelenting': 14205,\n",
       " 'horribly': 2354,\n",
       " \"'manufactured\": 35704,\n",
       " 'edendale': 63612,\n",
       " 'sheilas': 62911,\n",
       " 'cooter': 42169,\n",
       " 'miyan': 51569,\n",
       " 'aloud': 13559,\n",
       " 'infecting': 32883,\n",
       " 'victories': 37388,\n",
       " 'chitty': 84079,\n",
       " 'kafi': 66395,\n",
       " 'reshaping': 57996,\n",
       " '7300': 50977,\n",
       " 'repairman': 28154,\n",
       " 'bluegrass': 33057,\n",
       " 'priestesses': 31464,\n",
       " 'trajectories': 46155,\n",
       " 'more': 50,\n",
       " 'hermione': 21302,\n",
       " 'beutiful': 50441,\n",
       " 'forestry': 46075,\n",
       " 'pokeball': 44311,\n",
       " 'cragg': 25921,\n",
       " 'whips': 13742,\n",
       " 'life': 110,\n",
       " 'metropolis': 8986,\n",
       " 'transporter': 25930,\n",
       " 'miscalculated': 47819,\n",
       " 'o’connor': 71734,\n",
       " 'fangs': 18808,\n",
       " 'rennaissance': 56218,\n",
       " 'lowish': 68712,\n",
       " \"reggie's\": 49427,\n",
       " 'ramble': 14407,\n",
       " 'cartouche': 39525,\n",
       " 'malnutrition': 83987,\n",
       " 'pint': 23306,\n",
       " 'bazaar': 61776,\n",
       " 'hollyood': 54251,\n",
       " \"'don\": 34670,\n",
       " 'gains': 12454,\n",
       " \"d'artagnan\": 49490,\n",
       " 'alannis': 61997,\n",
       " 'lures': 13852,\n",
       " 'mantraps': 67815,\n",
       " \"enemy's\": 28882,\n",
       " 'clumsily': 9502,\n",
       " 'unladylike': 71629,\n",
       " 'puff': 19934,\n",
       " 'contents': 11042,\n",
       " 'fobby': 54514,\n",
       " \"mom'\": 87760,\n",
       " 'monthly': 27364,\n",
       " \"2000's\": 18492,\n",
       " 'disavow': 45452,\n",
       " 'ands': 24603,\n",
       " 'ratcheting': 36231,\n",
       " 'directoral': 73430,\n",
       " 'natal': 45761,\n",
       " 'faze': 34945,\n",
       " 'littlehammer16787': 57219,\n",
       " 'origonal': 59313,\n",
       " 'tinned': 54771,\n",
       " 'sue': 4717,\n",
       " \"candles'\": 68737,\n",
       " 'cosette': 23745,\n",
       " 'prognathous': 63510,\n",
       " 'instic': 57643,\n",
       " 'ream': 57891,\n",
       " 'kaplan': 32959,\n",
       " 'basball': 56441,\n",
       " 'acosta': 31961,\n",
       " 'accidental': 7762,\n",
       " \"perry's\": 15734,\n",
       " 'pharaohs': 63644,\n",
       " 'formed': 5947,\n",
       " 'hulu': 27572,\n",
       " 'zigzaggy': 67800,\n",
       " '274': 52029,\n",
       " 'rebukes': 79150,\n",
       " 'amovie': 56304,\n",
       " 'granny': 15649,\n",
       " 'bridgete': 45272,\n",
       " \"baron's\": 30039,\n",
       " 'fearlessly': 34870,\n",
       " 'cyncial': 85285,\n",
       " 'concubines': 37511,\n",
       " \"'must\": 25350,\n",
       " 'ballyhoo': 53800,\n",
       " 'shiven': 61244,\n",
       " 'jayhawkers': 25360,\n",
       " 'directeur': 84602,\n",
       " 'kelly\\x85': 52929,\n",
       " 'kittredge': 43717,\n",
       " 'lusterio': 56685,\n",
       " 'funnnny': 71161,\n",
       " 'slayride': 83967,\n",
       " \"groucho's\": 67044,\n",
       " 'sembene': 45489,\n",
       " 'unskillful': 72970,\n",
       " 'pontificates': 56767,\n",
       " 'chistina': 84918,\n",
       " 'splicing': 26173,\n",
       " \"garris's\": 76392,\n",
       " 'irksome': 22445,\n",
       " 'landon': 15960,\n",
       " 'palace': 5058,\n",
       " 'flagrante': 69221,\n",
       " \"collaborator'\": 53829,\n",
       " 'hike': 28982,\n",
       " 'bid': 14335,\n",
       " \"'merika\": 78727,\n",
       " 'dense': 9542,\n",
       " 'fabric': 8289,\n",
       " 'mosters': 63512,\n",
       " 'celluloidal': 58003,\n",
       " 'shipping': 30552,\n",
       " 'applauded': 10869,\n",
       " 'erotica': 15785,\n",
       " 'shorelines': 62868,\n",
       " 'marblehead': 35364,\n",
       " 'mistresses': 23568,\n",
       " 'babyface': 40680,\n",
       " 'incinerator': 34742,\n",
       " \"porky's\": 20056,\n",
       " 'bushranger': 54588,\n",
       " 'hutchence': 44447,\n",
       " 'lean': 6928,\n",
       " 'harbouring': 46905,\n",
       " 'superlative': 10400,\n",
       " 'witch\\x85': 80013,\n",
       " 'goran': 26540,\n",
       " \"'backbeat'\": 68940,\n",
       " 'girlishness': 76001,\n",
       " 'counterbalance': 38108,\n",
       " 'aintry': 49139,\n",
       " 'shorten': 32204,\n",
       " 'twangy': 44979,\n",
       " 'sorrel': 77442,\n",
       " \"'picnic\": 51193,\n",
       " 'dyana': 74153,\n",
       " 'doctrine': 19971,\n",
       " 'sacristan': 87576,\n",
       " 'maneuver': 22588,\n",
       " 'clocks': 18061,\n",
       " 'kevloun': 61163,\n",
       " 'antagonized': 52673,\n",
       " 'broz': 56739,\n",
       " 'moonshining': 58898,\n",
       " 'profit': 8886,\n",
       " 'massacrenot': 82756,\n",
       " 'sardo': 37499,\n",
       " \"'making\": 21610,\n",
       " \"tagawa's\": 63074,\n",
       " \"dostoyevski's\": 64429,\n",
       " 'tubbs': 26373,\n",
       " \"silence'\": 65428,\n",
       " 'scoobys': 74780,\n",
       " \"'eeriness'\": 64639,\n",
       " \"paper's\": 44153,\n",
       " 'prescott': 29165,\n",
       " 'mulgrew': 39287,\n",
       " 'snitch': 61416,\n",
       " 'exasperation': 24911,\n",
       " 'takita': 85326,\n",
       " 'laugher': 30484,\n",
       " 'dumont': 37089,\n",
       " 'thescreamonline': 63960,\n",
       " 'sarajevo': 46129,\n",
       " 'superlguing': 68939,\n",
       " 'redeemed': 9913,\n",
       " 'women\\x97adela': 82216,\n",
       " 'crooked': 8615,\n",
       " 'brittannica': 52104,\n",
       " 'evan': 16155,\n",
       " 'glug': 42447,\n",
       " 'episodes': 669,\n",
       " 'madhouse': 23443,\n",
       " 'borat': 22549,\n",
       " \"''the\": 20197,\n",
       " \"payal's\": 53342,\n",
       " \"'shogun\": 48201,\n",
       " 'joiner': 79655,\n",
       " \"hanks'\": 18774,\n",
       " 'borin': 85965,\n",
       " 'sossaman': 81583,\n",
       " 'overtime': 24259,\n",
       " 'momojiri': 52190,\n",
       " 'reactive': 48432,\n",
       " \"christy's\": 16662,\n",
       " 'flood': 10425,\n",
       " \"material's\": 52840,\n",
       " 'vitam': 73930,\n",
       " 'worringly': 76224,\n",
       " 'stopwatch': 47329,\n",
       " 'kilcher': 44580,\n",
       " 'cutters': 79031,\n",
       " \"cam't\": 64676,\n",
       " 'talbert': 23644,\n",
       " 'rupees': 50228,\n",
       " 'america': 935,\n",
       " '“him”': 65739,\n",
       " 'doofuses': 83802,\n",
       " 'limitation': 28295,\n",
       " 'petroichan': 74553,\n",
       " 'icewater': 63896,\n",
       " 'scudded': 81430,\n",
       " 'after': 100,\n",
       " 'unisols': 19699,\n",
       " 'ramones\\x85': 65777,\n",
       " 'memorial': 14372,\n",
       " 'lumpur': 60964,\n",
       " 'distortions': 17633,\n",
       " 'shore': 7534,\n",
       " 'discrepancy': 23146,\n",
       " 'giggle': 9223,\n",
       " 'dementia': 17602,\n",
       " 'reaction': 2094,\n",
       " 'garofalo': 14892,\n",
       " \"juice'\": 80483,\n",
       " 'hangouts': 83855,\n",
       " 'diggler': 46520,\n",
       " 'energised': 63440,\n",
       " \"almodóvar's\": 56224,\n",
       " 'unconcerned': 25997,\n",
       " 'movie\\x97even': 66180,\n",
       " 'hiphop': 29282,\n",
       " 'unscheduled': 73172,\n",
       " 'soled': 60263,\n",
       " 'drillings': 85507,\n",
       " 'catholicism': 24316,\n",
       " 'ganem': 82019,\n",
       " 'griped': 56543,\n",
       " 'salamat': 75910,\n",
       " 'torrences': 66238,\n",
       " 'denials': 51360,\n",
       " \"dream'\": 49881,\n",
       " 'exhalation': 52945,\n",
       " 'purged': 52149,\n",
       " 'shallowly': 41550,\n",
       " 'pov': 11145,\n",
       " 'seyrig': 56721,\n",
       " 'singled': 20615,\n",
       " 'leer': 29838,\n",
       " 'gaffari': 50110,\n",
       " \"dahmer'\": 44786,\n",
       " 'screenplay': 878,\n",
       " 'successive': 20979,\n",
       " 'ditch': 16009,\n",
       " 'comedy\\x85': 41899,\n",
       " 'nevada': 11440,\n",
       " 'kingpins': 87697,\n",
       " 'hollow': 4083,\n",
       " 'procreation': 77344,\n",
       " \"philipe's\": 62590,\n",
       " 'upswing': 58477,\n",
       " 'prettier': 15622,\n",
       " 'goddesses': 29325,\n",
       " 'noir': 1356,\n",
       " 'mistakingly': 42971,\n",
       " 'wiggling': 27513,\n",
       " 'zombied': 45419,\n",
       " 'vandervoort': 41283,\n",
       " \"cook's\": 22714,\n",
       " 'outliving': 82008,\n",
       " 'disregarded': 20547,\n",
       " 'bow\\x85so': 66639,\n",
       " 'dresser': 25447,\n",
       " 'specific': 3380,\n",
       " 'melodrama\\x85': 54518,\n",
       " 'reworked': 18559,\n",
       " \"hussein's\": 70487,\n",
       " 'karo': 35770,\n",
       " 'epitomé': 66932,\n",
       " 'aviation': 18942,\n",
       " 'frogs': 17342,\n",
       " \"kuriyama's\": 68596,\n",
       " \"hammett's\": 54477,\n",
       " 'phased': 49588,\n",
       " \"summerslam's\": 62549,\n",
       " \"early's\": 22393,\n",
       " 'microbudget': 57905,\n",
       " 'nerdy': 8934,\n",
       " 'forcibly': 18717,\n",
       " 'meets': 889,\n",
       " 'higginson': 43144,\n",
       " 'ahhhhh': 79946,\n",
       " 'tiananmen': 43010,\n",
       " 'andaaz': 63774,\n",
       " 'plodded': 29933,\n",
       " 'interlaced': 32756,\n",
       " \"'gardens\": 75510,\n",
       " 'clutters': 20351,\n",
       " 'tagge': 25804,\n",
       " \"reid's\": 21676,\n",
       " 'civl': 69093,\n",
       " 'nazgul': 26174,\n",
       " 'sematically': 87559,\n",
       " \"rockwell's\": 47793,\n",
       " 'jüri': 65957,\n",
       " 'cartwright': 14451,\n",
       " 'satisying': 80514,\n",
       " 'flanders': 21008,\n",
       " \"sinthasomphone's\": 61215,\n",
       " 'muxmäuschenstill': 58719,\n",
       " 'attention': 689,\n",
       " 'twentysomething': 26006,\n",
       " 'powdered': 43526,\n",
       " 'dominique': 16074,\n",
       " 'wynorski': 31433,\n",
       " 'oneness': 70624,\n",
       " 'bighouse': 73981,\n",
       " 'streamlined': 28056,\n",
       " 'bsers': 84701,\n",
       " 'venues': 24171,\n",
       " 'bearing': 7379,\n",
       " 'ancien': 63563,\n",
       " 'puppy': 6851,\n",
       " \"fidelity'\": 85451,\n",
       " 'mumbles': 16612,\n",
       " \"kitty's\": 36103,\n",
       " 'nooooo': 33161,\n",
       " 'barca': 51048,\n",
       " '¨abraham': 67036,\n",
       " 'ensued': 22958,\n",
       " 'schwarz': 37602,\n",
       " 'raring': 69628,\n",
       " 'smirk': 12793,\n",
       " 'hairline': 33270,\n",
       " 'colossally': 64653,\n",
       " 'rvd': 14236,\n",
       " 'trumillio': 54175,\n",
       " 'naughtier': 57785,\n",
       " 'glorious': 4146,\n",
       " 'beullar': 71752,\n",
       " 'cinemagic': 28932,\n",
       " \"'crime\": 61710,\n",
       " 'reporters': 12510,\n",
       " 'noise': 3358,\n",
       " \"'terrific'\": 85843,\n",
       " 'kasbah': 61849,\n",
       " 'shudder': 10828,\n",
       " 'diatribes': 26356,\n",
       " 'hallucinogens': 59899,\n",
       " \"brosnon's\": 53688,\n",
       " \"'sunday\": 86760,\n",
       " \"pacino's\": 10727,\n",
       " \"superheroes'\": 64887,\n",
       " 'whitechapel': 51291,\n",
       " 'unhumorous': 43047,\n",
       " 'litters': 67334,\n",
       " 'chor': 76278,\n",
       " 'attendees': 38571,\n",
       " 'exciter': 59323,\n",
       " \"screenin'\": 85105,\n",
       " \"anno's\": 55755,\n",
       " 'infesting': 40015,\n",
       " \"mile'\": 54573,\n",
       " 'auditoriums': 44840,\n",
       " \"'singh\": 66517,\n",
       " 'credibilty': 85989,\n",
       " \"hasn't\": 1478,\n",
       " 'slums': 15976,\n",
       " 'zombies\\x97natch': 60123,\n",
       " 'workaday': 43556,\n",
       " 'wrrrooonnnnggg': 58016,\n",
       " 'jorobado': 69732,\n",
       " 'rollup': 69911,\n",
       " 'mcgill': 49698,\n",
       " 'universes': 81629,\n",
       " 'aryian': 83299,\n",
       " \"'shrooms\": 64680,\n",
       " 'texture': 10027,\n",
       " 'gossamer': 74523,\n",
       " 'humanness': 35644,\n",
       " 'resemble': 5121,\n",
       " 'placidness': 53503,\n",
       " 'yong': 55299,\n",
       " 'troubadour': 30760,\n",
       " 'lamping': 55553,\n",
       " 'bound': 2722,\n",
       " 'interestig': 50658,\n",
       " 'straws': 20762,\n",
       " 'sticklers': 50793,\n",
       " 'memorized': 14928,\n",
       " 'gibberish': 12972,\n",
       " 'squandering': 33154,\n",
       " 'tightly': 7429,\n",
       " 'heaping': 21710,\n",
       " 'bem': 58369,\n",
       " 'shave': 14741,\n",
       " 'pvt': 27458,\n",
       " \"spiegel's\": 61039,\n",
       " 'jehan': 77616,\n",
       " 'efff': 83907,\n",
       " 'huffman': 38463,\n",
       " 'vertiginous': 40687,\n",
       " 'stacked': 25138,\n",
       " 'janus': 37329,\n",
       " 'kriemhild': 10807,\n",
       " 'hitch': 10277,\n",
       " 'cowardly': 8591,\n",
       " \"'exotic'\": 63549,\n",
       " 'lousiest': 34489,\n",
       " 'gorylicious': 56771,\n",
       " 'circulate': 35302,\n",
       " 'freinken': 76706,\n",
       " 'dribbled': 60298,\n",
       " 'button': 4555,\n",
       " \"whistler's\": 45961,\n",
       " 'staffer': 38021,\n",
       " 'prudish': 16971,\n",
       " \"mulder's\": 46692,\n",
       " 'allan': 8510,\n",
       " 'wtaf': 69393,\n",
       " 'outsleep': 62114,\n",
       " 'buffa': 70390,\n",
       " 'dolled': 35251,\n",
       " 'pratfalls': 19193,\n",
       " 'karnage': 44192,\n",
       " 'habousch': 86835,\n",
       " 'nauseous': 15667,\n",
       " 'weinbauer': 40725,\n",
       " 'cbn': 45321,\n",
       " 'tarri': 55761,\n",
       " \"cena's\": 38303,\n",
       " \"roy's\": 18066,\n",
       " 'godwin': 49850,\n",
       " \"nightmare's\": 74062,\n",
       " 'prairie': 14605,\n",
       " 'spasmodic': 82319,\n",
       " 'unsensationalized': 71848,\n",
       " 'squelching': 56522,\n",
       " 'characterizes': 30137,\n",
       " 'repetitevness': 60800,\n",
       " 'woodbury': 54485,\n",
       " 'ethiopia': 69655,\n",
       " \"emperor's\": 14836,\n",
       " 'car': 516,\n",
       " 'edelman': 29988,\n",
       " \"keanu's\": 42263,\n",
       " \"fricker's\": 42845,\n",
       " 'benefit\\x85not': 70434,\n",
       " 'friedo': 49824,\n",
       " \"lovely'\": 39044,\n",
       " 'kats': 81939,\n",
       " 'die\\x85': 71913,\n",
       " \"'reefer\": 37772,\n",
       " 'featurettes': 20542,\n",
       " 'beems': 65934,\n",
       " 'kleber': 82458,\n",
       " 'unfairness': 64985,\n",
       " 'triumphing': 26644,\n",
       " 'certo': 67453,\n",
       " \"wolf's\": 29410,\n",
       " 'arching': 84566,\n",
       " 'einon': 35256,\n",
       " 'cellmate': 75269,\n",
       " 'impeded': 49054,\n",
       " 'eugenic': 44586,\n",
       " 'clairvoyance': 21419,\n",
       " 'undergoing': 23285,\n",
       " 'muccino': 25532,\n",
       " 'duuum': 58098,\n",
       " 'masochism': 22441,\n",
       " 'thewlis': 18787,\n",
       " 'viard': 75763,\n",
       " 'indirection': 53710,\n",
       " \"morone's\": 56231,\n",
       " 'frodo': 12536,\n",
       " 'aggravate': 60968,\n",
       " 'giraudeau': 67986,\n",
       " 'shoplifter': 47952,\n",
       " 'misbehaviour': 83408,\n",
       " 'assimilate': 27995,\n",
       " 'givens': 35616,\n",
       " 'papamoschou': 33222,\n",
       " '260': 46653,\n",
       " 'aircraft': 8503,\n",
       " 'superbowl': 85618,\n",
       " \"'fantasy'\": 49853,\n",
       " 'manageable': 28014,\n",
       " 'swinging': 7027,\n",
       " 'unorthodox': 12891,\n",
       " \"consumerism'\": 51720,\n",
       " 'unrecycled': 56223,\n",
       " 'dursley': 57357,\n",
       " 'nris': 82011,\n",
       " 'carnotaur': 82518,\n",
       " 'hetero': 32041,\n",
       " 'screen\\x85': 70771,\n",
       " 'clients': 13632,\n",
       " 'léaud': 18276,\n",
       " 'inflated': 18504,\n",
       " 'workload': 38183,\n",
       " 'binds': 27521,\n",
       " 'emissary': 45753,\n",
       " \"'down\": 67010,\n",
       " 'oversees': 82142,\n",
       " 'imbred': 46296,\n",
       " 'ji': 14742,\n",
       " \"eddie's\": 13472,\n",
       " 'calf': 14094,\n",
       " 'janosch': 84948,\n",
       " 'spaniel': 49777,\n",
       " \"mahoney's\": 85596,\n",
       " 'lust': 4018,\n",
       " 'lenders': 42189,\n",
       " 'delarue': 58392,\n",
       " 'confrontations': 19053,\n",
       " 'cheaper': 9301,\n",
       " \"mode'\": 78465,\n",
       " \"piano'\": 78981,\n",
       " 'lofaso': 65743,\n",
       " 'imprisoning': 41498,\n",
       " 'mapes': 73720,\n",
       " 'hardnut': 63035,\n",
       " \"rivière's\": 53855,\n",
       " 'voluble': 63185,\n",
       " 'sharaff': 47000,\n",
       " 'plantation': 9852,\n",
       " 'bowdlerised': 51719,\n",
       " 'millionaire': 5285,\n",
       " 'saloons': 47302,\n",
       " 'whooshes': 75203,\n",
       " 'giada': 22065,\n",
       " 'potentialize': 71931,\n",
       " 'elaborate': 3758,\n",
       " 'shtick': 10269,\n",
       " '1000': 8134,\n",
       " 'inu': 49461,\n",
       " 'proby': 54294,\n",
       " 'inconsequentiality': 76347,\n",
       " 'prevents': 10005,\n",
       " 'marissa': 51071,\n",
       " 'nether': 33138,\n",
       " '1991': 6044,\n",
       " 'evict': 44041,\n",
       " \"ravings'\": 77557,\n",
       " 'elegius': 84086,\n",
       " 'karzis': 29521,\n",
       " 'lars': 8482,\n",
       " \"'realistically'\": 82084,\n",
       " 'spares': 23706,\n",
       " \"tomba's\": 72836,\n",
       " 'pulsing': 33564,\n",
       " 'sergei': 20544,\n",
       " 'littlefield': 21998,\n",
       " \"quartet'\": 53739,\n",
       " \"love'expresses\": 73070,\n",
       " 'scrapes': 18999,\n",
       " 'laraine': 37515,\n",
       " 'wuthering': 19020,\n",
       " 'cul': 47340,\n",
       " 'borrowing': 12761,\n",
       " 'dawkins': 25388,\n",
       " 'boarding': 10053,\n",
       " \"jannings's\": 44040,\n",
       " 'badger': 28269,\n",
       " 'lightheartedly': 45754,\n",
       " 'fic': 52080,\n",
       " 'magasiva': 67173,\n",
       " 'stipulates': 36798,\n",
       " 'katt': 27152,\n",
       " 'cavanaugh': 25250,\n",
       " 'bowen': 22512,\n",
       " 'snow': 3169,\n",
       " 'unkill': 79664,\n",
       " 'gangreen': 82564,\n",
       " 'atm': 50547,\n",
       " 'crowds': 9906,\n",
       " 'pullout': 55290,\n",
       " 'forebodings': 68815,\n",
       " 'bucketful': 63787,\n",
       " 'entitled': 5671,\n",
       " 'strawberry22': 84342,\n",
       " 'ivanna': 13707,\n",
       " 'conversational': 24041,\n",
       " 'rut': 17774,\n",
       " \"'unrequited\": 67680,\n",
       " \"piaf's\": 45316,\n",
       " 'comparision': 58547,\n",
       " 'aikens': 45151,\n",
       " 'sold': 2959,\n",
       " 'oklahoma': 12287,\n",
       " 'salaciousness': 53013,\n",
       " \"'70ies\": 53965,\n",
       " 'sleazes': 55440,\n",
       " 'bombsight': 83054,\n",
       " \"goody'\": 57087,\n",
       " 'communions': 53950,\n",
       " 'austion': 57585,\n",
       " 'vivaldi': 76379,\n",
       " 'kokanson': 86342,\n",
       " 'wvs': 52319,\n",
       " 'ungrammatical': 81649,\n",
       " 'julien': 21641,\n",
       " \"eglantine's\": 69167,\n",
       " 'remsen': 43998,\n",
       " 'muddles': 50423,\n",
       " 'gcse': 28051,\n",
       " 'crop': 9684,\n",
       " \"exorcist''\": 56480,\n",
       " \"'prayer'\": 85633,\n",
       " 'probability': 19689,\n",
       " 'plastrons': 80377,\n",
       " 'raffin': 27584,\n",
       " 'bach': 6681,\n",
       " 'molester': 17786,\n",
       " 'statement': 2587,\n",
       " 'bibles': 28234,\n",
       " 'biologist': 34165,\n",
       " 'felled': 50053,\n",
       " 'uav': 68281,\n",
       " 'enshrine': 76701,\n",
       " 'emotionally\\x85': 75453,\n",
       " 'disintegrates': 32649,\n",
       " \"rackham's\": 80889,\n",
       " 'grossest': 82180,\n",
       " 'minesweeper': 63942,\n",
       " 'himalayas': 18039,\n",
       " 'amazed': 2661,\n",
       " 'vocalise': 69660,\n",
       " \"businessman's\": 70241,\n",
       " '3yrs': 47413,\n",
       " 'mikshelt': 86955,\n",
       " 'crapola': 37106,\n",
       " 'halfassing': 52270,\n",
       " 'starships': 56321,\n",
       " 'acually': 53977,\n",
       " 'hisaichi': 51826,\n",
       " 'haply': 86379,\n",
       " 'kanji': 75749,\n",
       " \"'fills\": 75579,\n",
       " 'rucksack': 48996,\n",
       " 'huts': 29099,\n",
       " 'bisleri': 70563,\n",
       " 'inhumanities': 51166,\n",
       " 'ladylove': 60933,\n",
       " '152': 58297,\n",
       " 'sacraments': 86345,\n",
       " 'lage': 43264,\n",
       " '89s': 83333,\n",
       " 'washroom': 38818,\n",
       " 'softening': 30267,\n",
       " 'provocation': 21876,\n",
       " 'farcically': 85172,\n",
       " 'hundreds': 3100,\n",
       " 'reasserts': 78748,\n",
       " 'retains': 9997,\n",
       " \"gucht's\": 81405,\n",
       " 'delighted': 6636,\n",
       " 'contemplate': 9436,\n",
       " 'whitewash': 24453,\n",
       " 'blackens': 83743,\n",
       " 'alabama': 19878,\n",
       " 'lithgows': 63872,\n",
       " 'abt': 86757,\n",
       " 'waistline': 52388,\n",
       " 'mobster': 8755,\n",
       " 'mnm': 23011,\n",
       " 'repulsed': 17110,\n",
       " 'sufficiently': 11491,\n",
       " 'raymonde': 41614,\n",
       " \"'blackboard\": 61747,\n",
       " 'whopping': 17377,\n",
       " 'factoring': 64058,\n",
       " \"damiani's\": 82149,\n",
       " 'edinburgh': 12177,\n",
       " \"'awe\": 75964,\n",
       " 'chiffon': 84297,\n",
       " 'coupons': 32910,\n",
       " 'lk': 77819,\n",
       " 'purposes': 4932,\n",
       " 'bainter': 46584,\n",
       " 'mandela': 26020,\n",
       " 'halleck': 24817,\n",
       " 'sabertooth': 16983,\n",
       " 'puny': 27335,\n",
       " 'noltie': 66826,\n",
       " 'dormitories': 88399,\n",
       " 'filthiest': 61558,\n",
       " 'feckless': 24344,\n",
       " 'leaden': 13419,\n",
       " 'cast\\x97among': 63398,\n",
       " '\\x96arguably': 55143,\n",
       " 'verheyen': 43315,\n",
       " 'poldark': 86653,\n",
       " 'yasuzo': 54425,\n",
       " \"ranger'\": 71153,\n",
       " \"\\x91b'movie\": 86286,\n",
       " 'sole': 3716,\n",
       " 'urbibe': 62601,\n",
       " 'confederate': 10942,\n",
       " \"patinkin's\": 82072,\n",
       " 'ragnardocks': 71000,\n",
       " 'cycles': 23995,\n",
       " 'urineing': 87205,\n",
       " 'gorge': 30296,\n",
       " \"dress's\": 63586,\n",
       " 'stupidest': 9403,\n",
       " 'p45': 70442,\n",
       " 'snuffs': 86049,\n",
       " 'quota': 21473,\n",
       " 'smker': 61235,\n",
       " \"gale's\": 46042,\n",
       " 'redeemer': 42378,\n",
       " 'incursion': 80482,\n",
       " 'stillbirth': 59202,\n",
       " 'killling': 80310,\n",
       " 'quartered': 36240,\n",
       " \"'kunst\": 85701,\n",
       " 'ambersoms': 64199,\n",
       " 'pis': 49780,\n",
       " 'pablito': 43415,\n",
       " 'nonspecific': 85652,\n",
       " '2h30': 58407,\n",
       " 'globes': 21153,\n",
       " 'fieriest': 79322,\n",
       " 'fencing': 10658,\n",
       " 'aleksandar': 64288,\n",
       " 'orations': 53135,\n",
       " \"'ideological\": 58474,\n",
       " 'shinbei': 65305,\n",
       " 'matheisen': 66891,\n",
       " \"bekmambetov's\": 85203,\n",
       " \"same'\": 66958,\n",
       " 'bardem': 67767,\n",
       " \"lawrence's\": 26557,\n",
       " 'delegate': 66683,\n",
       " 'winfrey': 14456,\n",
       " 'argonautica': 78718,\n",
       " 'evolvement': 51218,\n",
       " 'fulci´s': 70887,\n",
       " 'desensitized': 20669,\n",
       " 'schweiger': 25322,\n",
       " 'noemi': 87455,\n",
       " 'livelier': 26661,\n",
       " \"silvio's\": 41797,\n",
       " 'plotting': 6516,\n",
       " 'lohan': 14329,\n",
       " \"princes'\": 85900,\n",
       " 'schumacher': 12024,\n",
       " 'duggan': 26527,\n",
       " 'hines': 7698,\n",
       " 'verges': 26448,\n",
       " 'baptism': 21600,\n",
       " 'duk': 29790,\n",
       " 'devincentis': 64467,\n",
       " 'tread': 18934,\n",
       " 'adeptly': 22814,\n",
       " 'cinematography\\x85': 79199,\n",
       " 'hajj': 38796,\n",
       " \"hoofer's\": 79500,\n",
       " 'hrabal': 62745,\n",
       " 'twee': 27315,\n",
       " 'affairs': 5622,\n",
       " 'formulate': 22972,\n",
       " 'corporatism': 50667,\n",
       " 'courted': 23232,\n",
       " 'medicinal': 42108,\n",
       " 'quedraogo': 69011,\n",
       " 'rub': 9690,\n",
       " 'meatiest': 80112,\n",
       " \"'sea\": 84689,\n",
       " 'idealists': 51711,\n",
       " 'kiel': 30018,\n",
       " '13': 1998,\n",
       " 'yoe': 80330,\n",
       " 'dedicates': 34671,\n",
       " 'absense': 52474,\n",
       " 'bierstube': 74315,\n",
       " 'anywho': 37857,\n",
       " 'already': 457,\n",
       " \"minded's\": 63410,\n",
       " 'rakastin': 55918,\n",
       " \"london's\": 11452,\n",
       " 'czekh': 71226,\n",
       " 'téchiné': 61830,\n",
       " \"northam's\": 33792,\n",
       " 'single': 683,\n",
       " 'training': 2330,\n",
       " \"mohanty's\": 69065,\n",
       " 'zombification': 28171,\n",
       " 'offensiveness': 36298,\n",
       " \"'ideas'\": 77928,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_index = dict([(value, key ) for (key, value) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([reverse_word_index.get(i -3, '?') for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the as you with out themselves powerful lets loves their becomes reaching had journalist of lot from anyone to have after out atmosphere never more room and it so heart shows to years of every never going and help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but and to story wonderful that in seeing in character to of 70s musicians with heart had shadows they of here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other and in of seen over landed for anyone of and br show's to whether from than out themselves history he name half some br of and odd was two most of mean for 1 any an boat she he should is thought frog but of script you not while history he heart to real at barrel but when from one bit then have two of script their with her nobody most that with wasn't to with armed acting watch an for with heartfelt film want an\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([reverse_word_index.get(i , '?') for i in train_data[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 778, 128, 74]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[5][0:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_review =' '.join([reverse_word_index.get(i -3 , '?') for i in train_data[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Decoded review the indices are offset by 3, because indices 0,1 and 2 are reserved for \"padding\", \"start of sequence,\" and \"unknown\"\n",
    "Which is why the sequence of words makes more sense than the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99, 6, 3]\n",
      "[2, 6, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "seq = [2,6,3,1]\n",
    "s = seq[0:3]\n",
    "s[0] = 99\n",
    "print(s)\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "indexing of a list is another list, indexing of a numpy array is a view\n",
    "# note \n",
    "there is such a thing a basic and advanced indexing... the intricate difference i haven't explored too much,\n",
    "but advanced indexing is a copy and basic is a view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[99  6  3]\n",
      "[99  6  3  1]\n"
     ]
    }
   ],
   "source": [
    "seq = np.array([2,6,3,1])\n",
    "s = seq[0:3]\n",
    "s[0] = 99\n",
    "print(s)\n",
    "print(seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating a one-hot vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension = 10000):\n",
    "    results  = np.zeros((len(sequences),dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 1. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the network\n",
    "\n",
    "The model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cardosoo/venv/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape = (10000,)))\n",
    "model.add(layers.Dense(16, activation = 'relu'))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'rmsprop', loss= 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting aside a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape\n",
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/cardosoo/venv/lib/python3.5/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size = 512, verbose = 0,validation_data = (x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_acc', 'loss', 'val_loss', 'acc'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history #contains entire history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.37968397860527037, 0.5084240040461222)\n",
      "(0.3003729674816132, 0.3004293824036916)\n",
      "(0.3085316753387451, 0.21790880763530732)\n",
      "(0.28397776350975035, 0.17504446516036987)\n",
      "(0.28414924812316894, 0.142692573984464)\n",
      "(0.3165922412395477, 0.11499823834896088)\n",
      "(0.31268108854293825, 0.09800445106029511)\n",
      "(0.38593706359863283, 0.08069882659912109)\n",
      "(0.36348215651512145, 0.0660659032980601)\n",
      "(0.3841077935218811, 0.05608599482774734)\n",
      "(0.41529247183799745, 0.043946201266845064)\n",
      "(0.45254025416374205, 0.03814133314689001)\n",
      "(0.4699304045677185, 0.029979005377491316)\n",
      "(0.5022735030174256, 0.024697823320825894)\n",
      "(0.5342192699432373, 0.017504934355119864)\n",
      "(0.5711044822692871, 0.014906830390791099)\n",
      "(0.6024912521362304, 0.015087408000727495)\n",
      "(0.6822317755699158, 0.007531567775209745)\n",
      "(0.6773126829147339, 0.012109525287648042)\n",
      "(0.6900547019958496, 0.00411029639840126)\n"
     ]
    }
   ],
   "source": [
    "val = zip(history_dict['val_loss'], history_dict['loss'])\n",
    "for i in val:\n",
    "    print(i) # val_loss, val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the training and validation loss\n",
    "# for some reason my kernel dies when i use matplotlib so i used a saved the data and plotted the values some in spyder :<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note your loss and accuracy graphs may look different because of initial randomizaiton\n",
    "import pickle\n",
    "#loss_values = history_dict['loss']\n",
    "#val_loss_values = history_dict['val_loss']\n",
    "#epochs = range(1, len(loss_values)+1)\n",
    "pickle_out = open(\"imdb_history\", \"wb\")\n",
    "pickle.dump(history_dict,pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plt.plot(epochs, loss_values, 'bo', label='Training loss') #bo for blue dots\n",
    "#plt.plot(epochs, val_loss_values, 'b', label = 'Validation loss')#b for blue lines\n",
    "#plt.title('Training and validation loss')\n",
    "#plt.xlabel('Epochs')\n",
    "#plt.ylabel('Loss')\n",
    "#plt.legend()\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retraining a model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "25000/25000 [==============================] - 1s 49us/step - loss: 0.4750 - acc: 0.8219\n",
      "Epoch 2/4\n",
      "25000/25000 [==============================] - 1s 44us/step - loss: 0.2657 - acc: 0.9096\n",
      "Epoch 3/4\n",
      "25000/25000 [==============================] - 1s 45us/step - loss: 0.1983 - acc: 0.9294\n",
      "Epoch 4/4\n",
      "25000/25000 [==============================] - 1s 44us/step - loss: 0.1677 - acc: 0.9402\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation='relu', input_shape = (10000,)))\n",
    "model.add(layers.Dense(16, activation = 'relu'))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "model.compile(optimizer = 'rmsprop', loss= 'binary_crossentropy', metrics = ['accuracy'])\n",
    "history2 = model.fit(x_train, y_train, epochs=4, batch_size = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 49us/step\n",
      "[0.3245329585170746, 0.87312]\n",
      "['loss', 'acc']\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(x_test, y_test)\n",
    "print(results)\n",
    "print(model.metrics_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using train network to generate predictions on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13865885],\n",
       "       [0.99969566],\n",
       "       [0.28815672],\n",
       "       ...,\n",
       "       [0.07008764],\n",
       "       [0.04285681],\n",
       "       [0.46925393]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1\n",
    "Build models with 1, 3 and 4 hiddden layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "layerz = [1,3,4]\n",
    "networks = []\n",
    "for model in range(len(layerz)):\n",
    "    networks.append(models.Sequential())\n",
    "    networks[model].add(layers.Dense(16, activation='relu', input_shape = (10000,)))\n",
    "    for layer in range(layerz[model]):\n",
    "        networks[model].add(layers.Dense(16, activation = 'relu'))\n",
    "    networks[model].add(layers.Dense(1, activation = 'sigmoid'))\n",
    "    networks[model].compile(optimizer = 'rmsprop', loss= 'binary_crossentropy', metrics = ['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 5, 3)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(networks[2].layers),len(networks[1].layers),len(networks[0].layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in range(len(layerz)):\n",
    "    networks[model].compile(optimizer = 'rmsprop', loss= 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fit models on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/4\n",
      "15000/15000 [==============================] - 2s 124us/step - loss: 0.0238 - acc: 0.9968 - val_loss: 0.4952 - val_acc: 0.8687\n",
      "Epoch 2/4\n",
      "15000/15000 [==============================] - 2s 120us/step - loss: 0.0198 - acc: 0.9973 - val_loss: 0.4923 - val_acc: 0.8716\n",
      "Epoch 3/4\n",
      "15000/15000 [==============================] - 2s 109us/step - loss: 0.0173 - acc: 0.9970 - val_loss: 0.5260 - val_acc: 0.8698\n",
      "Epoch 4/4\n",
      "15000/15000 [==============================] - 2s 109us/step - loss: 0.0091 - acc: 0.9995 - val_loss: 0.5672 - val_acc: 0.8620\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/4\n",
      "15000/15000 [==============================] - 4s 262us/step - loss: 0.5845 - acc: 0.7339 - val_loss: 0.4309 - val_acc: 0.8625\n",
      "Epoch 2/4\n",
      "15000/15000 [==============================] - 2s 127us/step - loss: 0.3237 - acc: 0.8954 - val_loss: 0.3123 - val_acc: 0.8773\n",
      "Epoch 3/4\n",
      "15000/15000 [==============================] - 2s 121us/step - loss: 0.2149 - acc: 0.9280 - val_loss: 0.2890 - val_acc: 0.8837\n",
      "Epoch 4/4\n",
      "15000/15000 [==============================] - 2s 116us/step - loss: 0.1720 - acc: 0.9408 - val_loss: 0.3086 - val_acc: 0.8768\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/4\n",
      "15000/15000 [==============================] - 4s 239us/step - loss: 0.5683 - acc: 0.7557 - val_loss: 0.4178 - val_acc: 0.8676\n",
      "Epoch 2/4\n",
      "15000/15000 [==============================] - 2s 116us/step - loss: 0.3246 - acc: 0.8923 - val_loss: 0.3160 - val_acc: 0.8787\n",
      "Epoch 3/4\n",
      "15000/15000 [==============================] - 2s 118us/step - loss: 0.2214 - acc: 0.9223 - val_loss: 0.2799 - val_acc: 0.8904\n",
      "Epoch 4/4\n",
      "15000/15000 [==============================] - 2s 115us/step - loss: 0.1701 - acc: 0.9413 - val_loss: 0.3321 - val_acc: 0.8785\n"
     ]
    }
   ],
   "source": [
    "historys = []\n",
    "for model in range(len(layerz)):\n",
    "    historys.append(networks[model].fit(partial_x_train, \n",
    "                                         partial_y_train, epochs=4, #i choose 4 epochs because in the above model it was best\n",
    "                                         batch_size = 512, \n",
    "                                         validation_data = (x_val,y_val)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'val_loss', 'val_acc', 'loss', 'acc' for all 3 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.5671663908481598 0.8619999994277954 0.009095460824668408 0.8619999994277954\n",
      "2\n",
      "0.308602543592453 0.8767999999046325 0.1720100563287735 0.8767999999046325\n",
      "3\n",
      "0.3320854649066925 0.8785000001907348 0.17009066094557443 0.8785000001907348\n"
     ]
    }
   ],
   "source": [
    "final = len(historys[0].history['val_loss']) -1\n",
    "for i in range(len(layerz)):\n",
    "    print(str(i + 1))\n",
    "    print(historys[i].history['val_loss'][final],historys[i].history['val_acc'][final],historys[i].history['loss'][final],historys[i].history['val_acc'][final])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 layers had the most accuraccy but 3 hidden layers had the least loss... not sure why i thought the correlation between validation loss and accuracy would be 1.0. Anyways i'll pick 4 cos i guess accuracy is more important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# side experiment ... (trying layers with sizes 6 ,7,9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/4\n",
      "15000/15000 [==============================] - 9s 575us/step - loss: 0.6008 - acc: 0.7203 - val_loss: 0.5071 - val_acc: 0.7944\n",
      "Epoch 2/4\n",
      "15000/15000 [==============================] - 3s 169us/step - loss: 0.4104 - acc: 0.8930 - val_loss: 0.3715 - val_acc: 0.8805\n",
      "Epoch 3/4\n",
      "15000/15000 [==============================] - 2s 118us/step - loss: 0.2519 - acc: 0.9292 - val_loss: 0.3400 - val_acc: 0.8619\n",
      "Epoch 4/4\n",
      "15000/15000 [==============================] - 2s 118us/step - loss: 0.1738 - acc: 0.9423 - val_loss: 0.3407 - val_acc: 0.8816\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/4\n",
      "15000/15000 [==============================] - 5s 318us/step - loss: 0.6234 - acc: 0.7055 - val_loss: 0.5211 - val_acc: 0.8564\n",
      "Epoch 2/4\n",
      "15000/15000 [==============================] - 2s 127us/step - loss: 0.4374 - acc: 0.8867 - val_loss: 0.4222 - val_acc: 0.8745\n",
      "Epoch 3/4\n",
      "15000/15000 [==============================] - 2s 134us/step - loss: 0.2983 - acc: 0.9211 - val_loss: 0.3547 - val_acc: 0.8793\n",
      "Epoch 4/4\n",
      "15000/15000 [==============================] - 3s 195us/step - loss: 0.1888 - acc: 0.9381 - val_loss: 0.3193 - val_acc: 0.8832\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/4\n",
      "15000/15000 [==============================] - 7s 489us/step - loss: 0.6061 - acc: 0.7123 - val_loss: 0.5003 - val_acc: 0.8638\n",
      "Epoch 2/4\n",
      "15000/15000 [==============================] - 2s 148us/step - loss: 0.4495 - acc: 0.8949 - val_loss: 0.5440 - val_acc: 0.8357\n",
      "Epoch 3/4\n",
      "15000/15000 [==============================] - 2s 142us/step - loss: 0.3774 - acc: 0.9174 - val_loss: 0.4898 - val_acc: 0.8721\n",
      "Epoch 4/4\n",
      "15000/15000 [==============================] - 2s 119us/step - loss: 0.2697 - acc: 0.9347 - val_loss: 0.3747 - val_acc: 0.8629\n",
      "1\n",
      "0.34073496289253236 0.8815999998092652 0.17378581624825795 0.8815999998092652\n",
      "2\n",
      "0.3193161252975464 0.8831999998092651 0.188788800851504 0.8831999998092651\n",
      "3\n",
      "0.374716881608963 0.8628999994277954 0.2697079610188802 0.8628999994277954\n"
     ]
    }
   ],
   "source": [
    "layerz = [6,8,9]\n",
    "networks = []\n",
    "for model in range(len(layerz)):\n",
    "    networks.append(models.Sequential())\n",
    "    networks[model].add(layers.Dense(16, activation='relu', input_shape = (10000,)))\n",
    "    for layer in range(layerz[model]):\n",
    "        networks[model].add(layers.Dense(16, activation = 'relu'))\n",
    "    networks[model].add(layers.Dense(1, activation = 'sigmoid'))\n",
    "    networks[model].compile(optimizer = 'rmsprop', loss= 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "for model in range(len(layerz)):\n",
    "    networks[model].compile(optimizer = 'rmsprop', loss= 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "historys = []\n",
    "for model in range(len(layerz)):\n",
    "    historys.append(networks[model].fit(partial_x_train, \n",
    "                                         partial_y_train, epochs=4, #i choose 4 epochs because in the above model it was best\n",
    "                                         batch_size = 512, \n",
    "                                         validation_data = (x_val,y_val)))\n",
    "final = len(historys[0].history['val_loss']) -1\n",
    "for i in range(len(layerz)):\n",
    "    print(str(i + 1))\n",
    "    print(historys[i].history['val_loss'][final],historys[i].history['val_acc'][final],historys[i].history['loss'][final],historys[i].history['val_acc'][final])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interesting observation... 8 layers had the most accuracy... fuck it imma pick 8 layes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2\n",
    "trying out different hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = [32,64,128]\n",
    "layerz = 8\n",
    "networks = []\n",
    "for model in range(len(units)):\n",
    "    networks.append(models.Sequential())\n",
    "    networks[model].add(layers.Dense(units[model], activation='relu', input_shape = (10000,)))\n",
    "    for layer in range(layerz):\n",
    "        networks[model].add(layers.Dense(units[model], activation = 'relu'))\n",
    "    networks[model].add(layers.Dense(1, activation = 'sigmoid'))\n",
    "    networks[model].compile(optimizer = 'rmsprop', loss= 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/4\n",
      "15000/15000 [==============================] - 9s 595us/step - loss: 0.5785 - acc: 0.7047 - val_loss: 0.3838 - val_acc: 0.8525\n",
      "Epoch 2/4\n",
      "15000/15000 [==============================] - 3s 170us/step - loss: 0.3183 - acc: 0.8799 - val_loss: 0.3618 - val_acc: 0.8556\n",
      "Epoch 3/4\n",
      "15000/15000 [==============================] - 2s 133us/step - loss: 0.2224 - acc: 0.9203 - val_loss: 0.3367 - val_acc: 0.8696\n",
      "Epoch 4/4\n",
      "15000/15000 [==============================] - 2s 132us/step - loss: 0.1723 - acc: 0.9389 - val_loss: 0.5416 - val_acc: 0.8058\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/4\n",
      "15000/15000 [==============================] - 6s 395us/step - loss: 0.5443 - acc: 0.7251 - val_loss: 0.3513 - val_acc: 0.8620\n",
      "Epoch 2/4\n",
      "15000/15000 [==============================] - 3s 172us/step - loss: 0.2826 - acc: 0.8891 - val_loss: 0.2900 - val_acc: 0.8837\n",
      "Epoch 3/4\n",
      "15000/15000 [==============================] - 3s 170us/step - loss: 0.1953 - acc: 0.9237 - val_loss: 0.3955 - val_acc: 0.8571\n",
      "Epoch 4/4\n",
      "15000/15000 [==============================] - 3s 174us/step - loss: 0.1392 - acc: 0.9484 - val_loss: 0.3839 - val_acc: 0.8804\n",
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/4\n",
      "15000/15000 [==============================] - 7s 495us/step - loss: 0.5737 - acc: 0.7253 - val_loss: 0.3168 - val_acc: 0.8774\n",
      "Epoch 2/4\n",
      "15000/15000 [==============================] - 4s 247us/step - loss: 0.2668 - acc: 0.8936 - val_loss: 0.2979 - val_acc: 0.8874\n",
      "Epoch 3/4\n",
      "15000/15000 [==============================] - 4s 248us/step - loss: 0.1916 - acc: 0.9277 - val_loss: 0.4299 - val_acc: 0.8513\n",
      "Epoch 4/4\n",
      "15000/15000 [==============================] - 4s 250us/step - loss: 0.1183 - acc: 0.9564 - val_loss: 0.2864 - val_acc: 0.8872\n",
      "1\n",
      "0.5415851084709168 0.8057999996185303 0.17232653064727782 0.8057999996185303\n",
      "2\n",
      "0.38390573854446414 0.8803999998092651 0.13915774693489075 0.8803999998092651\n",
      "3\n",
      "0.28638739318847656 0.8872000001907349 0.11825140884717306 0.8872000001907349\n"
     ]
    }
   ],
   "source": [
    "for model in range(len(units)):\n",
    "    networks[model].compile(optimizer = 'rmsprop', loss= 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "historys = []\n",
    "for model in range(len(units)):\n",
    "    historys.append(networks[model].fit(partial_x_train, \n",
    "                                         partial_y_train, epochs=4, #i choose 4 epochs because in the above model it was best\n",
    "                                         batch_size = 512, \n",
    "                                         validation_data = (x_val,y_val)))\n",
    "final = len(historys[0].history['val_loss']) -1\n",
    "for i in range(len(units)):\n",
    "    print(str(i + 1))\n",
    "    print(historys[i].history['val_loss'][final],historys[i].history['val_acc'][final],historys[i].history['loss'][final],historys[i].history['acc'][final])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.5415851084709168 0.8057999996185303 0.17232653064727782 0.9389333331743877\n",
      "2\n",
      "0.38390573854446414 0.8803999998092651 0.13915774693489075 0.9484000000635783\n",
      "3\n",
      "0.28638739318847656 0.8872000001907349 0.11825140884717306 0.9564000001271565\n"
     ]
    }
   ],
   "source": [
    "final = len(historys[0].history['val_loss']) -1\n",
    "for i in range(len(units)):\n",
    "    print(str(i + 1))\n",
    "    print(historys[i].history['val_loss'][final],historys[i].history['val_acc'][final],historys[i].history['loss'][final],historys[i].history['acc'][final])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the more the hidden units the better the model... Therefore i shall choose 128 hidden units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Experiment 3\n",
    "different loss function: mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 128\n",
    "layerz = 8\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units, activation='relu', input_shape = (10000,)))\n",
    "for layer in range(layerz):\n",
    "    network.add(layers.Dense(units, activation = 'relu'))\n",
    "network.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "network.compile(optimizer = 'rmsprop', loss= 'mse', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/4\n",
      "15000/15000 [==============================] - 8s 543us/step - loss: 0.1897 - acc: 0.7271 - val_loss: 0.1162 - val_acc: 0.8501\n",
      "Epoch 2/4\n",
      "15000/15000 [==============================] - 4s 249us/step - loss: 0.0808 - acc: 0.8912 - val_loss: 0.1021 - val_acc: 0.8594\n",
      "Epoch 3/4\n",
      "15000/15000 [==============================] - 4s 248us/step - loss: 0.0568 - acc: 0.9260 - val_loss: 0.0961 - val_acc: 0.8706\n",
      "Epoch 4/4\n",
      "15000/15000 [==============================] - 4s 256us/step - loss: 0.0335 - acc: 0.9591 - val_loss: 0.0921 - val_acc: 0.8855\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'history'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-587541a2affb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                          validation_data = (x_val,y_val))\n\u001b[1;32m      5\u001b[0m \u001b[0mfinal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhistorys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'history'"
     ]
    }
   ],
   "source": [
    "history = network.fit(partial_x_train, \n",
    "                                         partial_y_train, epochs=4, #i choose 4 epochs because in the above model it was best\n",
    "                                         batch_size = 512, \n",
    "                                         validation_data = (x_val,y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09209611726999282 0.8855000003814697 0.03346091506977876 0.9590666665712992\n"
     ]
    }
   ],
   "source": [
    "final = len(history.history['val_loss']) -1\n",
    "print(history.history['val_loss'][final],\n",
    "      history.history['val_acc'][final],\n",
    "      history.history['loss'][final],\n",
    "      history.history['acc'][final])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "binary_crossentropy:0.28638739318847656 0.8872000001907349 0.11825140884717306 0.9564000001271565 is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Experiment 4\n",
    "different activation function: tanh\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15000 samples, validate on 10000 samples\n",
      "Epoch 1/4\n",
      "15000/15000 [==============================] - 10s 642us/step - loss: 0.6770 - acc: 0.6402 - val_loss: 0.3417 - val_acc: 0.8632\n",
      "Epoch 2/4\n",
      "15000/15000 [==============================] - 5s 325us/step - loss: 0.3350 - acc: 0.8683 - val_loss: 0.2859 - val_acc: 0.8839\n",
      "Epoch 3/4\n",
      "15000/15000 [==============================] - 5s 353us/step - loss: 0.2382 - acc: 0.9080 - val_loss: 0.4118 - val_acc: 0.8413\n",
      "Epoch 4/4\n",
      "15000/15000 [==============================] - 5s 359us/step - loss: 0.2053 - acc: 0.9212 - val_loss: 0.3554 - val_acc: 0.8517\n",
      "0.3554300384998322 0.851699999332428 0.20534951910972596 0.9211999997456869\n"
     ]
    }
   ],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units, activation='tanh', input_shape = (10000,)))\n",
    "for layer in range(layerz):\n",
    "    network.add(layers.Dense(units, activation = 'tanh'))\n",
    "network.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "network.compile(optimizer = 'rmsprop', loss= 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "history = network.fit(partial_x_train, \n",
    "                                         partial_y_train, epochs=4, #i choose 4 epochs because in the above model it was best\n",
    "                                         batch_size = 512, \n",
    "                                         validation_data = (x_val,y_val))\n",
    "final = len(history.history['val_loss']) -1\n",
    "print(history.history['val_loss'][final],\n",
    "      history.history['val_acc'][final],\n",
    "      history.history['loss'][final],\n",
    "      history.history['acc'][final])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "relu:0.28638739318847656 0.8872000001907349 0.11825140884717306 0.9564000001271565 is better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# final Experiment\n",
    "evaluation using testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units, activation='relu', input_shape = (10000,)))\n",
    "for layer in range(layerz):\n",
    "    network.add(layers.Dense(units, activation = 'relu'))\n",
    "network.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "network.compile(optimizer = 'rmsprop', loss= 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "history = network.fit(partial_x_train, \n",
    "                                         partial_y_train, epochs=4, #i choose 4 epochs because in the above model it was best\n",
    "                                         batch_size = 512, \n",
    "                                         validation_data = (x_val,y_val))\n",
    "final = len(history.history['val_loss']) -1\n",
    "print(history.history['val_loss'][final],\n",
    "      history.history['val_acc'][final],\n",
    "      history.history['loss'][final],\n",
    "      history.history['acc'][final])\n",
    "\n",
    "results = model.evaluate(x_test, y_test)\n",
    "print(results)\n",
    "print(model.metrics_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
