{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to join all the prior and post together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_in = open(\"Best 10 features in Actor\", \"rb\")\n",
    "actor_indices = pickle.load(pickle_in)\n",
    "pickle_in.close()\n",
    "pickle_in1 = open(\"Best 10 features in Partner\", \"rb\")\n",
    "partner_indices = pickle.load(pickle_in1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "missing_values = [\"n/a\", \"na\", \"--\", \" \"]\n",
    "actor_train = pd.read_csv(\"/home/cardosoo/492_19summer/MachineLearning/Data/Training actor.csv\", na_values = missing_values )\n",
    "actor_test = pd.read_csv(\"/home/cardosoo/492_19summer/MachineLearning/Data/Testing actor.csv\" , na_values = missing_values )\n",
    "partner_train = pd.read_csv(\"/home/cardosoo/492_19summer/MachineLearning/Data/Testing partner.csv\" , na_values = missing_values )\n",
    "partner_test = pd.read_csv(\"/home/cardosoo/492_19summer/MachineLearning/Data/Testing partner.csv\" , na_values = missing_values )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Given train and test this function finds intersecting columns names\n",
    "returns a sorted lowercase list of the intersecting column names\n",
    "\"\"\"\n",
    "def intersect_columsn(train_df,test_df):\n",
    "    train = train_df.fillna(0)\n",
    "    test = test_df.fillna(0)\n",
    "    train.isna().sum().sum(),test.isna().sum().sum()\n",
    "\n",
    "    # columns may have same names but different cases\n",
    "    train_cols = [x.lower() for x in train.columns.tolist()] \n",
    "    test_cols = [x.lower() for x in test.columns.tolist()]\n",
    "    print(len(train_cols))\n",
    "    print(len(test_cols))\n",
    "    A = set(train_cols)\n",
    "    B = set(test_cols)\n",
    "    C = A & B\n",
    "    C = A & B\n",
    "\n",
    "    len(C)\n",
    "\n",
    "    print(\"Non intersecting column names: \")\n",
    "    print(\"A-C\",A - C)\n",
    "    print(\"B-C\",B-C)\n",
    "    D = A - C\n",
    "    E = B - C\n",
    "    Not_C = D | E\n",
    "    for x in Not_C:\n",
    "        if x in train_cols:\n",
    "            train_cols.remove(x)\n",
    "        if x in test_cols:\n",
    "            test_cols.remove(x)\n",
    "    print(len(train_cols),len(test_cols))\n",
    "\n",
    "    train_X = train_cols[0:len(train_cols)-1]# not including digging actor\n",
    "    y = train_cols[len(train_cols)-1]\n",
    "    test_X = test_cols[0:len(test_cols)-1]\n",
    "    train_X.sort()\n",
    "    test_X.sort()\n",
    "    train_X.append(y)\n",
    "    test_X.append(y) \n",
    "    print(train_X == test_X)\n",
    "\n",
    "    train_cols = train.columns.tolist() \n",
    "    test_cols = test.columns.tolist()\n",
    "    # making the column names lowercase so we can index the features we want\n",
    "    train_cols = [x.lower() for x in train_cols]\n",
    "    test_cols = [x.lower() for x in test_cols]\n",
    "\n",
    "    #reassigning column names\n",
    "     \n",
    "    return (train_cols, test_cols),(train_X, test_X)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n",
      "85\n",
      "Non intersecting column names: \n",
      "A-C set()\n",
      "B-C set()\n",
      "85 85\n",
      "True\n",
      "87\n",
      "87\n",
      "Non intersecting column names: \n",
      "A-C set()\n",
      "B-C set()\n",
      "87 87\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "(actor_train.columns, actor_test.columns), (train_X, test_X) = intersect_columsn(actor_train,actor_test)\n",
    "actor_train = actor_train[train_X]\n",
    "actor_test = actor_test[test_X]\n",
    "(partner_train.columns, partner_test.columns), (train_X, test_X) = intersect_columsn(partner_train,partner_test)\n",
    "partner_train = partner_train[train_X]\n",
    "partner_test = partner_test[test_X]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now i have to find the column names associated with the top 10 features for actor and partner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "CR = list(actor_indices[0])\n",
    "actor_CR_feature = [actor_test.columns.tolist()[i]for i in CR]\n",
    "CR = list(partner_indices[0])\n",
    "partner_CR_feature = [partner_test.columns.tolist()[i]for i in CR]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['agreeable',\n",
       " 'anxious',\n",
       " 'growth',\n",
       " 'indepselconstrual',\n",
       " 'init2',\n",
       " 'init3',\n",
       " 'ipdom',\n",
       " 'ipearnambit',\n",
       " 'ipintell',\n",
       " 'ipphyshot']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partner_CR_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asintell',\n",
       " 'fourminpliked',\n",
       " 'fourminwarm',\n",
       " 'init1',\n",
       " 'ludus',\n",
       " 'rejectionsen',\n",
       " 'satpeop',\n",
       " 'sexor',\n",
       " 'storge',\n",
       " 'youint_1']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor_CR_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_actor_train = pd.read_csv(\"/home/cardosoo/492_19summer/MachineLearning/Data/Level 2 Post-Interaction predicting Actor, Sample B.csv\", na_values = missing_values )\n",
    "post_actor_test = pd.read_csv(\"/home/cardosoo/492_19summer/MachineLearning/Data/Level 2 Post-Interaction predicting Actor, Sample B.csv\" , na_values = missing_values )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"Best 10 Post features in Actor\", \"rb\")\n",
    "post_actor_indices = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "42\n",
      "Non intersecting column names: \n",
      "A-C set()\n",
      "B-C set()\n",
      "42 42\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "(post_actor_train.columns, post_actor_test.columns), (train_X, test_X) = intersect_columsn(post_actor_train,post_actor_test)\n",
    "post_actor_train = post_actor_train[train_X]\n",
    "post_actor_test = post_actor_test[test_X]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chemisactorgm',\n",
       " 'dominactorgm',\n",
       " 'intellactorgm',\n",
       " 'pdiggingactorgm',\n",
       " 'percpopactorgm',\n",
       " 'physhotactorgm',\n",
       " 'psanxactorgm',\n",
       " 'psavoidactorgm',\n",
       " 'vitalityactorgm',\n",
       " 'warmthactorgm']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CR = list(post_actor_indices[0])\n",
    "post_actor_CR_feature = [post_actor_test.columns.tolist()[i]for i in CR]\n",
    "post_actor_CR_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1092,) (1092, 36) (1092, 37)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((2050, 37), (1092, 37), (958, 37))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_X), len(test_X )\n",
    "train = train[train_X]\n",
    "test = test[test_X]\n",
    "train_data = train.values.astype('float')\n",
    "test_data = test.values.astype('float')\n",
    "cols = train_data.shape[1]\n",
    "y = train_data[:,cols-1]\n",
    "X = train_data[:,0:cols-1]\n",
    "print(y.shape,X.shape, train_data.shape)\n",
    "\n",
    "# Joining sample A and sample B together\n",
    "full_data = np.concatenate((train_data,test_data))\n",
    "full_data.shape,train_data.shape,test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from keras import layers\n",
    "from keras import models\n",
    "\"\"\"\n",
    "Builds a nn regression modelall_val_mae_histories, all_val_loss_histories, all_loss_histories, all_mae_histories\n",
    "\"\"\"\n",
    "def build_model(units,layerz,metric,shape):\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(units, activation = 'relu', \n",
    "                           input_shape = (shape,)))\n",
    "    for layer in range(layerz):\n",
    "        model.add(layers.Dense(units, activation = 'relu'))\n",
    "    model.add(layers.Dense(1)) #linear layer\n",
    "    model.compile(optimizer = 'rmsprop', loss = 'mse', metrics = metric  )\n",
    "    return model\n",
    "\n",
    "\"\"\"\n",
    "params: units, layerz, metric,num_epochs\n",
    "returns : mae, loss, validation loss and validition mae histories for each fold\n",
    "          tuple( all_val_mae_histories, all_val_loss_histories, all_loss_histories, all_mae_histories)\n",
    "\"\"\"\n",
    "def validaton(hyper_parameters,shape,data):\n",
    "    k = 4\n",
    "    \n",
    "    units, layerz = hyper_parameters['units'], hyper_parameters['layerz']\n",
    "    metric,num_epochs = hyper_parameters['metric'], hyper_parameters['num_epochs']\n",
    "    train_data,train_targets = data['train'],data['target']\n",
    "    \n",
    "    num_val_samples = len(train_data) // k\n",
    "    all_val_mae_histories = []\n",
    "    all_val_loss_histories = []\n",
    "    all_loss_histories = []\n",
    "    all_mae_histories = []\n",
    "    all_val_mape_histories = []\n",
    "    \n",
    "    print(\"val_samples: \" + str(num_val_samples))\n",
    "    for i in range(k):\n",
    "        print('\\tprocessing fold #', i)\n",
    "        start = i*num_val_samples\n",
    "        stop = (i + 1) * num_val_samples\n",
    "        val_data = train_data[start:stop]\n",
    "        val_target = train_targets[start:stop]\n",
    "\n",
    "        partial_train_data = np.concatenate( (train_data[:start], train_data[stop:]) ,axis = 0)\n",
    "        partial_train_target = np.concatenate( (train_targets[:start],train_targets[stop:]), axis = 0) \n",
    "\n",
    "        model = build_model(units, layerz, metric,shape)\n",
    "        history = model.fit(partial_train_data, partial_train_target, epochs = num_epochs, batch_size = 1, verbose = 0,\n",
    "                            validation_data = (val_data, val_target) )\n",
    "\n",
    "\n",
    "        val_mae_history = history.history['val_mean_absolute_error']\n",
    "        val_loss_history = history.history['val_loss']\n",
    "        loss_history = history.history['loss']\n",
    "        mae_history = history.history['mean_absolute_error']\n",
    "        val_mape_history = history.history['val_mean_absolute_percentage_error']\n",
    "        \n",
    "        all_val_mae_histories.append(val_mae_history)  \n",
    "        all_val_loss_histories.append(val_loss_history)\n",
    "        all_loss_histories.append(loss_history)\n",
    "        all_mae_histories.append(mae_history)\n",
    "        all_val_mape_histories.append(val_mape_history)\n",
    "        \n",
    "    return all_val_mae_histories, all_val_loss_histories, all_loss_histories, all_mae_histories,all_val_mape_histories\n",
    "\n",
    "\"\"\"\n",
    "returns the histories of all the P * K folds\n",
    "\"\"\"\n",
    "def iterated_validation(num_iterations,hyper_parameters,shape,data):\n",
    "    all_val_mae_histories = []\n",
    "    all_val_loss_histories = []\n",
    "    all_loss_histories = []\n",
    "    all_mae_histories = []\n",
    "    all_val_mape_histories = []\n",
    "    train_data,train_targets = data['train'],data['target']\n",
    "    for i in range(num_iterations):\n",
    "        print(\"iteration: \"+str(i+1))\n",
    "            # shuffle training data\n",
    "        rows = np.arange(train_targets.size)\n",
    "        indexes = shuffle(rows)\n",
    "        train_data = train_data[indexes]\n",
    "        train_targets = train_targets[indexes]\n",
    "        print(\"\\tStarting indexes for training \" +str(indexes[0:5]))\n",
    "        \n",
    "        histories = validaton(hyper_parameters,shape,data)\n",
    "        all_val_mae_histories.extend(histories[0])\n",
    "        all_val_loss_histories.extend(histories[1])\n",
    "        all_loss_histories.extend(histories[2])\n",
    "        all_mae_histories.extend(histories[3])\n",
    "        all_mape_histories.extend(histories[4])\n",
    "        \n",
    "    return all_val_mae_histories, all_val_loss_histories, all_loss_histories, all_mae_histories,all_val_mape_histories\n",
    "\n",
    "\"\"\"\n",
    "Returns the mean for each elements returned in by the \n",
    "iterated_validation or validation function\n",
    "\"\"\"\n",
    "def average_folds(all_val_mae_histories, all_val_loss_histories, all_loss_histories, all_mae_histories,all_val_mape_histories):\n",
    "    averages = []\n",
    "    num_epochs = len(all_val_mae_histories[0])\n",
    "    averages.append([np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)])\n",
    "    averages.append([np.mean([x[i] for x in all_val_mae_histories]) for i in range(num_epochs)])\n",
    "    averages.append([np.mean([x[i] for x in all_loss_histories]) for i in range(num_epochs)])\n",
    "    averages.append([np.mean([x[i] for x in all_val_loss_histories]) for i in range(num_epochs)])\n",
    "    averages.append([np.mean([x[i] for x in all_val_mape_histories]) for i in range(num_epochs)])\n",
    "    return averages\n",
    "\n",
    "\"\"\"\n",
    "Creates and plots 4 subplots. That represents \n",
    "the loss and Mae for the training and validation set\n",
    "\"\"\"\n",
    "def plots(histories):\n",
    "    val_mae = 1\n",
    "    val_loss = 3\n",
    "    val_mape = 4\n",
    "    indices = [val_mae,val_mape]\n",
    "    fig, axes = plt.subplot(1,2,figsize=(12,12))\n",
    "    axes.reshape((2,))\n",
    "    fig.title('overfitting with different size features')\n",
    "    fig.set_xlabel('Epochs')\n",
    "    fig.set_ylabel('Validation mae')\n",
    "    color = [\"b\",\"r\"]\n",
    "    name = [\"MAE\",\"MAPE\"]\n",
    "    for i , axis in enumerate(axes):\n",
    "        axis.plot(histories[indices[i]], colour[i], label)\n",
    "        plt.legend()\n",
    "    print(name + \" had a \" + str(np.min(histories[val_mae])) +\" MAE value\")\n",
    "    print(name + \" had a \" + str(np.max(histories[val_mape])) +\" MAPE value\")\n",
    "    print(name + \" had a \" + str(np.min(histories[val_loss])) +\" Loss value\")\n",
    "    print()\n",
    "    \n",
    "    \n",
    "def plot_features(features, names, smooth=0):\n",
    "    val_mae_index = 1\n",
    "    val_loss_index = 3\n",
    "    epochs = range(1,len(features[names[0]][val_mae_index])+1)\n",
    "    color = [\"b\",\"r\",\"g\",\"k\"]\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.title('overfitting with different size features')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Validation mae')\n",
    "    for i,name in enumerate(names):\n",
    "        if smooth != 0:\n",
    "            plt.plot(epochs, smooth_curve(features[names[i]][val_mae_index]), color[i], label=name) \n",
    "        else:\n",
    "            plt.plot(epochs, features[names[i]][val_mae_index], color[i], label=name) \n",
    "        plt.legend()\n",
    "        print(name + \" had a \" + str(np.min(features[names[i]][val_mae_index])) +\" MAE value\")\n",
    "        print(name + \" had a \" + str(np.min(features[names[i]][val_loss_index])) +\" Loss value\")\n",
    "        print()\n",
    "    plt.show()\n",
    "    \n",
    "\"\"\"\n",
    "Manipulates the data such that it looks smoother when you plot it\n",
    "\"\"\"\n",
    "def smooth_curve(points, factor=0.9):\n",
    "    smoothed_points = []\n",
    "    for point in points:\n",
    "        if smoothed_points:\n",
    "            previous = smoothed_points[-1]\n",
    "            smoothed_points.append(previous * factor + point*(1 - factor))\n",
    "        else:\n",
    "            smoothed_points.append(point)\n",
    "    return smoothed_points\n",
    "\n",
    "\n",
    "def normalize(X, indices):\n",
    "    ratio = 0.8\n",
    "    split = int(np.floor(X.shape[0]*ratio))\n",
    "    normalized_train_datas = []\n",
    "    normalized_test_datas = []\n",
    "    mean = X[0:split,:].mean(axis = 0)\n",
    "    normalized_X = X.copy()\n",
    "    normalized_X[0:split,:] -= mean\n",
    "    std = X.std(axis = 0)\n",
    "    normalized_X /= std\n",
    "\n",
    "\n",
    "    for i in range(len(indices)): # should change this the test_data has information about the whole dataset\n",
    "        normalized_train_datas.append(normalized_X[0:split,indices[i]])\n",
    "        normalized_test_datas.append(normalized_X[split:,indices[i]])\n",
    "    return normalized_train_datas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1:\n",
    "units = 48, layerz = 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1\n",
      "\tStarting indexes for training [637 382 464 184 243]\n",
      "val_samples: 218\n",
      "\tprocessing fold # 0\n"
     ]
    }
   ],
   "source": [
    "num = 10 #num of selected features\n",
    "#corrolation, Mutual Information, Mean decrease impurities ,Mean decrease accuracy\n",
    "train_datas = []\n",
    "test_datas = []\n",
    "ratio = 0.8\n",
    "split = int(np.floor(X.shape[0]*ratio))\n",
    "\n",
    "hyper_parameters = dict(units = 48, layerz = 2, metric =['mae','mape'], num_epochs = 100)\n",
    "shape = X.shape[1]\n",
    "train_data = X[0:split,:]\n",
    "train_target = y[0:split]\n",
    "data = dict(train = train_data, target = train_target)\n",
    "\n",
    "histories = iterated_validation(1, hyper_parameters,shape,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note your loss and accuracy graphs may look different because of initial randomizaiton\n",
    "pickle_out = open(\"PostExperiment1RM\", \"wb\")\n",
    "pickle.dump(features,pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# pickle_in = open(\"PostExperiment1RM\", \"rb\")\n",
    "# features = pickle.load(pickle_in)\n",
    "# pickle_in.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average = average_folds(all_val_mae_histories, all_val_loss_histories, all_loss_histories, all_mae_histories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plots(average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
